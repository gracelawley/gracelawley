---
title: "Math Meets Data"
subtitle: "Transitioning from math proofs to natural language research"
author: "Grace Lawley"
date: "April 4th, 2019"
output: 
  xaringan::moon_reader:
    css: ["default", "css/my-fonts.css", "css/my-theme.css"]
    lib_dir: libs
    nature:
      countIncrementalSlides: false
      highlightStyle: github
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE, 
                      dpi = 300, cache = TRUE, fig.align = "center", 
                      out.width = "80%", fig.path = "figures/")

library(tidyverse)
library(broom)
library(knitr)
library(here)
library(tidytext)
library(gutenbergr)
```

class: middle

### About me

Graduated from Lewis & Clark College in 2017

* Majored in Math

Computer Science & Engineering PhD student at OHSU
* September 2017 - now

* Halfway through my 2nd year!

--


.pull-left[
Academic Interests

+ Computational Linguistics

+ Natural Language Processing

+ Speech and Language Disorders
]
.pull-right[
<br>
+ Discrete Math, Statistics

+ Data Science, Data Visualization

+ R, Python
]
---
class: middle

### About me

Graduated from Lewis & Clark College in 2017

* Majored in Math

Computer Science & Engineering PhD student at OHSU
* September 2017 - now

* Halfway through my 2nd year!


.pull-left[
Academic Interests

+ .look[Computational Linguistics?]

+ .look[Natural Language Processing?]

+ Speech and Language Disorders
]
.pull-right[
<br>
+ Discrete Math, Statistics

+ .look[Data Science?], Data Visualization

+ R, Python
]

---
class: middle

.fancy[Natural Language Processing (NLP)]

* .emphasize[Natural Language]

> "Any human language that has evolved naturally in a community, usually in contrast to computer programming languages or to artificially constructed languages such as Esperanto." <br> .sidebar[Wiktionary]

--

* .emphasize[Natural Language Processing]

> "...a subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular .emphasize[how to program computers to process and analyze large amounts of natural language data]." <br> .sidebar[Wikipedia]

---
class: middle

.fancy[Computational Linguistics]

> "...the scientific study of language from a computational perspective. Computational linguists are interested in .emphasize[providing computational models of various kinds of linguistic phenomena]. These models may be "knowledge-based" ("hand-crafted") or "data-driven" ("statistical" or "empirical")." <br> .sidebar[Association for Computational Linguistics]

---
class: middle

.fancy[Computational Linguistics]

> "...the scientific study of language from a computational perspective. Computational linguists are interested in providing computational models of various kinds of linguistic phenomena. These models may be .emphasize["knowledge-based"] ("hand-crafted") or .emphasize["data-driven"] ("statistical" or "empirical")." <br> .sidebar[Association for Computational Linguistics]



---
class: middle

.fancy[Data Science]*


> "..a multi-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from structured and unstructured data." <br> .sidebar[Wikipedia]

.footnote[.smaller[One out of __many__ definitions]]

---
class: middle, center, inverse

## How did I get here?

![](images/ilana1.gif)


---

__2009 - 2013__

* High school in Oakland, CA

* Know I like math & language...can I combine them?

--

* At the end of my Senior year, I found out about .fancy[computational linguistics] (thanks wikipedia!)

--

__2013 - 2014__
  + New York University
  
  + Took my first linguistics class

--

__2014 - 2015 __

  + Transferred to Lewis & Clark College
  
  + Started as a computer science major...

---

__2014 - 2015 __
  + ...took Calculus II, and then switched to math
  
--
  
  + Translation Theory & Practice
  
  + Calculus III, Linear Algebra, Discrete Math
  
--

__2015 - 2016__
  
  + Independent study: "Introduction to Computational Linguistics"
  
  + Math Colloquium Talk: "Atypical Language in Autism: Can we measure it?"
  
  + Met Jan van Santen (director of CSLU & my future advisor)
  
  + Summer internship at CSLU

--

__2016 - 2017__
  
  + Number Theory, Prob/Stats I & II

  + CSLU Internship #2
  
  + Applied for the PhD program at CSLU

---
class: middle

### What I do

Center for Spoken Language Understanding, OHSU

  + Automatic speech recognition, image processing, augmentative and alternative communication devices, ...
  
  + Computer Science & Electrical Engineering Master's & PhD programs
  
--

Funded by NIH grant

  + _Automated Measurement of Language Outcomes for Neurodevelopmental Disorders, R01DC012033_
  
  + Autism Spectrum Disorder (ASD), Fragile X Syndrome (FXS), Down Syndrome (DS)

---
class: middle

### I'm also...

Finishing up coursework requirements, here's what I've taken so far:

.pull-left[
+ Analyzing Sequences

+ Probability & Statistical Inference for Scientists and Engineers

+ Data Science Programming

+ Introduction to Linguistics & Communication Disorders

+ Principles & Practices of Data Visualization
]

.pull-right[
+ Artificial Intelligence

+ Algorithms

+ Natural Language Processing

+ Research Ethics in Computer Science

+ Machine Learning
]

<br>
<br>
.sidebar[If you want to talk about undergrad vs. grad school ask me during the Q&A, or come to the GemSTEM panel today at 6:30 pm!]
---
class: center, middle, inverse

## Research

---
class: middle

### My Current Research

Characteristics of Autism Spectrum Disorder (ASD)
    
  + Restricted, repetitive interests
  
  + Difficulties with social communication

--

.fancy[Pedantic speech]

  + Overly formal, adult-like speech
  
  + Using conventional words and phrases in unusual and peculiar ways
  
  + Jill Dolata's example:
    
    + "I ate shrimp for lunch"
    
    + "I ate crustaceans for lunch"
  
---
class: middle

### How is this currently measured?

#### Autism Diagnostic Observation Schedule (ADOS)
  
  + Standard ASD assessment tool
  
  + Series of semi-structured, examiner led activities
  
  + Coding scheme for behaviors characteristic of ASD 

--
  
  + Pedantic speech:
  
  > Use of words or phrases tends to be more repetitive or .look[formal] than that of most individuals at the same level of expressive language, but not obviously odd...

--

#### Limitations

* Subjective, inconsistent across examiners 

* Based on observational reports from parents/teachers/clinicians

---
class: middle

### Alternatives?

* Could try to develop an automated method to quantify this
    * ...but capturing something so subtle will be difficult

* How can you teach a computer to differentiate "I ate shrimp from lunch" from "I ate crustaceans for lunch?"

* Lots of different ways to be pedantic
  + Vocabulary choice
  + Style of speaking
  + Tone

* Usually want to smooth outliers, but in this case the outliers are the points we are interested in

--

 .emphasize[Q: Can pedantic speech in children with ASD be measured and described by using natural language processing methods?]

---
class: inverse, center, middle

## Language $\rightarrow$ Computer

![](images/life-size.gif)

---
class: middle

### Language is complicated...

--

Excerpts from a corpus of "confusing or misleading headlines" <sup>1<sup>

```{r echo=FALSE}
library(rcorpora)
headlines <- corpora("words/crash_blossoms")$crash_blossoms

headlines[2]
headlines[13]
headlines[14]
headlines[19]
headlines[21]
headlines[24]
headlines[31]
```
.footnote[[1] corpus: https://github.com/dariusk/corpora]
--
<br>
"`r headlines[19]`"     
* the hospital is sued by doctors who are 7 feet tall?
* the hospital is sued by seven podiatrists?  
<br>
<br>
<br>



---
class: middle

.emphasize[How do we teach a computer to understand language?]
* Need to represent language in a form that we can analyze computationally/mathematically/statistically

* Simplify language and represent it numerically

--

.emphasize[First step:] break apart the language into discrete units

  + Written text is made up of .emphasize[sentences]

  + Speech is made up of .emphasize[utterances]

--

.emphasize[We can break apart sentences and utterances even further into...]

---
class: middle

.fancy[Tokens]

* "A token is .emphasize[a meaningful unit of text], such as a word, that we are interested in using for analysis" - Text Mining with R

* "A rose is a rose" $\rightarrow$ [a, rose, is, a, rose]

--

* "aren't" $\rightarrow$ [are, n't], [are, not], [are, nt], or [aren't] ?

--

.fancy[Types]

 * "A rose is a rose" $\rightarrow$ [a, rose, is]

---
class: middle

.fancy[n-grams]  

* .emphasize[unigrams]: "A rose is a rose" $\rightarrow$ [a, rose, is, a, rose]

--

* .emphasize[bigrams]: "A rose is a rose" $\rightarrow$ [(a, rose), (rose is), (is, a), (a, rose)]

--

* .emphasize[trigrams]: "A rose is a rose" $\rightarrow$ [a, rose, is), (rose, is, a),...,(is, a, rose)]

--

.fancy[Morphemes]

* The smallest grammatical unit

* "unbreakable" $\rightarrow$ un - break - able

* "dogs" $\rightarrow$ dog - s

* "buyer" $\rightarrow$ buy - er
---
class: middle, inverse, center

## What can we use these for?

---
class: middle

### Q: How much do they talk?

* Can measure this with .fancy[Mean Length of Utterance (MLU)]

* MLU = average number of morphemes per utterance for a sample of 100 utterances

* Used as a measure of expressive language, language productivity, and language development

  + _Expressive language_ is language used to communicate, while _receptive language_ is the ability to understand language
  
  + All else being equal, a higher MLU reflects a higher level of language proficiency
  
* As a typically developing child gets older, they will produce longer and more complex utterances

---

### MLU in Language Development


__Brown's Stages of Syntactic and Morphological Development__ <sup> 1,2 <sup>

|Stage|Age (months)|MLU|Example Utterances|
|:---|:---|:---|:---|
|I|12 - 26|1.0 - 2.0| "that car" <br> "more juice"|
|II|27 - 30|2.0 - 2.5| "it going" <br > "in box" <br> "my cars"|
|III|31 - 34|2.5 - 3.0| "man's book" <br> "was it Alison?"|
|IV|35 - 40|3.0 - 3.75| "the puppy chews it" <br> "a ball on the book" |
|V|47 + |4.5 + | "were you hungry?" <br> "we're hiding" |


.footnote[[1] Brown, R. (1973). _A First Language: The Early Stages._ London: George Allen & Unwin. <br>
[2] https://www.speech-language-therapy.com]
---

class: middle

### Q: How repetitive are they being?

* Can talk a lot while not really saying much 

  + e.g. "row row row your boat..."

* One way to measure degree of lexical variation is with .fancy[Type-Token Ratio (TTR)]

* $\text{ttr} = \frac{\text{# unique words}}{\text{total # words}}$

* A higher TTR reflects a more diverse vocabulary

* If someone says a lot but is very repetitive they will have a lower TTR

---
```{r echo=FALSE}
# baa baa black sheep
sheep <- c("Baa, baa, black sheep,", "Have you any wool?", "Yes sir, yes sir,", 
           "Three bags full.", "One for the master,", "One for the dame,",
           "And one for the little boy", "Who lives down the lane.", 
           "Baa, baa, black sheep,", "Have you any wool?", "Yes sir, yes sir,",
           "Three bags full.", "One to mend the jerseys", "one to mend the socks",
           "and one to mend the holes in", "the little girls' frocks.",
           "Baa, baa, black sheep,", "Have you any wool?", "Yes sir, yes sir,",
           "Three bags full.")

sheep_df <- tibble(lines = sheep) %>% 
  unnest_tokens(word, lines) %>% 
  add_count(word, sort = TRUE) %>% 
  distinct(word, n, .keep_all = TRUE)

sheep_tokens <- sum(sheep_df$n)
sheep_types <- nrow(sheep_df)

# shakespeare sonnet 18
sonnets <- corpora("words/literature/shakespeare_sonnets")[[2]]
sonnet_18 <- sonnets$lines[18][[1]]

sonnnet_df <- tibble(lines = sonnet_18) %>% 
  unnest_tokens(word, lines) %>% 
  add_count(word, sort = TRUE) %>% 
  distinct(word, n, .keep_all = TRUE)

sonnet_tokens <- sum(sonnnet_df$n)
sonnet_types <- nrow(sonnnet_df)
```

.pull-left[
Baa Baa Black Sheep
.smaller[
Baa, baa, black sheep,  
Have you any wool?  
Yes sir, yes sir,  
Three bags full.  

One for the master,  
One for the dame,  
And one for the little boy  
Who lives down the lane.  

Baa, baa, black sheep,  
Have you any wool?  
Yes sir, yes sir,  
Three bags full.  

One to mend the jerseys  
one to mend the socks  
and one to mend the holes in  
the little girls' frocks.  

Baa, baa, black sheep,  
Have you any wool?  
Yes sir, yes sir,  
Three bags full.  
]
]

.pull-right[
Shakespeare's Sonnet XVIII
.smaller[
Shall I compare thee to a summer’s day?  
Thou art more lovely and more temperate:  
Rough winds do shake the darling buds of May,  
And summer’s lease hath all too short a date;  
Sometime too hot the eye of heaven shines,  
And often is his gold complexion dimm'd;  
And every fair from fair sometime declines,  
By chance or nature’s changing course untrimm'd;  
But thy eternal summer shall not fade,  
Nor lose possession of that fair thou ow’st;  
Nor shall death brag thou wander’st in his shade,  
When in eternal lines to time thou grow’st:  
&nbsp;&nbsp;So long as men can breathe or eyes can see,  
&nbsp;&nbsp;So long lives this, and this gives life to thee.  
]
]

--

|title|# types|# tokens|type-token-ratio|
|:---|:---|:---|:---|
|Baa Baa Black Sheep|`r sheep_types`|`r sheep_tokens`|`r sheep_types/sheep_tokens`|
|Sonnet XVII|`r sonnet_types`| `r sonnet_tokens`|`r sonnet_types/sonnet_tokens`|



---
class: middle

### Q: _What_ are they talking about?

* One way to capture this is by counting the number of occurences of each type that appears

* Can do this with a .fancy[Term-Document Matrix]

  + A matrix of frequency counts for each term (i.e. type) in every document

  + .emphasize[Language] $\rightarrow$ .emphasize[matrix of numbers] $\rightarrow$ .emphasize[linear algebra!]

---
class: middle

### Language $\rightarrow$ Term-Document Matrix

```{r echo=FALSE}
shakespeare <- corpora("words/literature/shakespeare_phrases")[[2]]

shakespeare2 <- shakespeare[2]
shakespeare3 <- shakespeare[3]
shakespeare9 <- shakespeare[9]
shakespeare48 <- shakespeare[48]
```

.pull-left[
.emphasize[Phrases coined by Shakespeare:]

"`r shakespeare2`"  
"`r shakespeare3`"
]
.pull-right[
<br>
<br>
"`r shakespeare9`"  
"`r shakespeare48`"
]

--

.emphasize[Term-Document Matrix:]
```{r echo = FALSE}
shakespeare_df <- tibble(`D1` = shakespeare2,
                         `D2` = shakespeare9, 
                         `D3` = shakespeare48,
                         `D4` = shakespeare3) %>% 
  gather(doc, term, D1, D2, D3, D4) %>% 
  group_by(doc) %>% 
  unnest_tokens(term, term) %>% 
  add_count(term) 

term_doc <- shakespeare_df %>% 
  distinct(term, .keep_all = TRUE) %>% 
  ungroup(doc) %>% 
  complete(doc, term, fill = list(n=0)) %>% 
  spread(term, n) %>% 
  column_to_rownames(var = "doc") %>% 
  as.matrix()


term_doc
```


.emphasize[Limitations]

* Term-Document Matrices are typically sparse and high-dimensional (we will circle back to this later)

* Counts for commonly used words like "and", "is", and "the" (a.k.a. _stop words_) will be inflated

  + Could remove these words before analysis
  + Could use a weighting scheme like .emphasize[tf-idf]!



---
class: middle

.fancy[Term Frequency-Inverse Document Frequency (tf-idf)]

+ "....emphasize[tf-idf] is intended to measure how important a word is to a document in a collection (or corpus) of documents, for example, to one novel in a collection of novels or to one website in a collection of websites." <sup>1<sup>

* $\text{tf-idf} = \text{tf}(term, doc) \times \text{idf}(term)$
  
  + $\text{idf}(term) = ln(\frac{n_{\text{documents}}}{n_{\text{documents containing term}}})$

  + Term frequency = frequency of $t$ in $d$ 
  
  + Inverse document frequency = how "important" $t$ is across all documents
    
+ Identifies the words that are used a lot in a document but are not too common overall

  + Words that are frequently used across all documents will have a _lower weight_ while words that are rarely used overall will have a _higher weight_


.footnote[[1] https://www.tidytextmining.com/tfidf.html]
---
class: middle

.emphasize[Term-Document Matrix using raw counts]
```{r echo=FALSE}
term_doc
```


.emphasize[Term-Document Matrix using tf-idf]
```{r echo=FALSE}
total_shakespeare <- shakespeare_df %>% 
  group_by(doc) %>% 
  mutate(total = sum(n)) %>% 
  ungroup()

shakespeare_tf_idf <- total_shakespeare %>% 
  bind_tf_idf(term, doc, n)

# shakespeare_tf_idf %>% 
#   distinct(term, idf) %>% 
#   arrange(idf)
```

```{r echo=FALSE}
shakespeare_tf_idf %>% 
  select(doc, term, tf_idf) %>% 
  group_by(doc) %>% 
  distinct(term, .keep_all = TRUE) %>% 
  ungroup() %>% 
  complete(doc, term, fill = list(tf_idf=0)) %>% 
  spread(term, tf_idf) %>% 
  column_to_rownames(var = "doc") %>% 
  as.matrix() %>% 
  round(2) 
  #kable(format="html")

```

---
class: inverse, center, middle

## How can we use these tools <br> to measure pedantic speech?

---
class: middle
### Measuring pedantic speech


Quick recap

* Lots of different ways to be pedantic

  + Vocabulary choice
  + Style of speaking
  + Tone
  
Approach

* Focus on pedantic speech that is caused by .emphasize[vocabulary choice]

* Hypothesis: Using infrequent/uncommon words $\rightarrow$ pedantic speech

* Create a document-term matrix where a document corresponds to a participant

* Limitations: 
  + Might not capture 2+ word phrases that are pedantic


---


### Back to Pedantic Speech

__The data I have__

  + Transcriptions of interactions between examiner and child (ADOS)
  
  + Add n's for total transcripts and n's for dx breakdown - ERPA data
  
__Preprocessing__

  + Convert all letters to lowercase
  
  + Remove all coded words - e.g. "xxx and I went to the park"
  
  + Keep contractions as is - e.g. "don't" vs. "do not"
   
  + Tokenize


---

### Explore pedantry at unigram level

* Create a term-document matrix

  + Each row in a token (term)
  
  + Each column is a child
  
  + Each row, column corresponds to the number of times a child said a token

--

* Issues...

  + Lots of zero counts! 
  
  + High dimensional matrix

--


* Approach
  
  1. $\log_{10}(x+1)$ transformation for the zero counts and skew
  
  2. Explore dimensionality reduction methods

---

### To do

Slides:
* Curse of dimensionality
* Overview of some dimensionality reduction techniques
* Examples of each with visualizations
* Applying MDS to ERPA data
* Evaluation
* CSEE program

To add:
* ERPA metadata summary statistics
* 
---

